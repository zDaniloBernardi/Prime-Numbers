{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKDTHiaIGmKixhqu+7I4cK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zDaniloBernardi/Prime-Numbers/blob/Valida%C3%A7%C3%A3o-de-Parametros/Valida%C3%A7%C3%A3o_de_Par%C3%A2metros_te%C3%B3ricos_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "An√°lise Preditiva de Gaps entre N√∫meros Primos\n",
        "\n",
        "Este script realiza uma an√°lise dos gaps entre n√∫meros primos consecutivos\n",
        "com o objetivo de prever caracter√≠sticas do pr√≥ximo gap.\n",
        "Ele inclui:\n",
        "- Gera√ß√£o de primos e gaps.\n",
        "- An√°lise de estacionariedade.\n",
        "- Diferencia√ß√£o (inteira e fracion√°ria - ARFIMA).\n",
        "- Engenharia de features (estat√≠sticas robustas em janela, FFT, event window).\n",
        "- Modelagem preditiva com XGBoost.\n",
        "- Valida√ß√£o cruzada temporal e avalia√ß√£o de performance.\n",
        "- Testes estat√≠sticos para compara√ß√£o de modelos.\n",
        "- An√°lise de import√¢ncia de features.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZBQVbGhSPgnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fracdiff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeQyKQ44QDn2",
        "outputId": "89be6b74-25a3-43c8-9079-760dcfdb05bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.9.0 Requires-Python >=3.7.12,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement fracdiff (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for fracdiff\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------\n",
        "# Importa√ß√µes de Bibliotecas\n",
        "# --------------------------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sympy import primerange, ntheory # ntheory para estimar o n-√©simo primo\n",
        "import math\n",
        "from scipy.stats import median_abs_deviation\n",
        "from scipy.signal import detrend, get_window\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from fracdiff import Fracdiff # Para diferencia√ß√£o fracion√°ria: pip install fracdiff\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.inspection import permutation_importance\n",
        "import xgboost as xgb\n",
        "from scipy.stats import wilcoxon # Para teste de Wilcoxon\n",
        "from sklearn.utils import resample # Para Bootstrap CI\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# ‚öôÔ∏è CONFIGURA√á√ïES GLOBAIS\n",
        "# --------------------------------------------------------------------------\n",
        "# --- Configura√ß√µes de Dados ---\n",
        "N_PRIMES = 1_000_000  # N√∫mero de primos a serem gerados\n",
        "# Estimativa para o limite superior da gera√ß√£o de primos (PNT: p_n ~ n log n)\n",
        "# Usaremos uma estimativa mais folgada para garantir que temos N_PRIMES\n",
        "PRIME_UPPER_BOUND_ESTIMATE = int(N_PRIMES * (math.log(N_PRIMES) + math.log(math.log(N_PRIMES)) * 1.5)) if N_PRIMES > 1000 else 15_485_864 # (p_1M √© ~1.5e7)\n",
        "\n",
        "# --- Configura√ß√µes de Diferencia√ß√£o e ARFIMA ---\n",
        "# Se True, aplica diferencia√ß√£o. 'integer' ou 'fractional'.\n",
        "APPLY_DIFFERENCING = True\n",
        "DIFFERENCING_TYPE = 'fractional' # 'integer' ou 'fractional'\n",
        "INTEGER_DIFF_ORDER = 1 # Ordem para diferencia√ß√£o inteira (d=1)\n",
        "# Par√¢metro 'd' para diferencia√ß√£o fracion√°ria (baseado no roadmap R3.2, d‚âà0.215)\n",
        "# Este valor pode ser ajustado/otimizado.\n",
        "FRACTIONAL_DIFF_D = 0.215\n",
        "\n",
        "# --- Configura√ß√µes de Features em Janela Deslizante ---\n",
        "WINDOW_SIZE_STATS = 20 # Para Median_Gap, MAD_Gap (Roadmap R2.1)\n",
        "STATS_STEP = 1 # Passo para janelas de estat√≠sticas (pode ser ajustado para reduzir correla√ß√£o ou computa√ß√£o)\n",
        "\n",
        "WINDOW_SIZE_FFT = 20 # Janela para FFT (Roadmap R4.1)\n",
        "FFT_STEP = WINDOW_SIZE_FFT // 2 # Overlap de 50% para FFT (Roadmap R4.1)\n",
        "FFT_TOP_K = 3 # N√∫mero de componentes FFT a extrair (Roadmap R4.1)\n",
        "FFT_WINDOW_TYPE = 'hann' # Tipo de janela para FFT (ex: 'hann', 'hamming')\n",
        "FFT_APPLY_DETREND = True # Se True, aplica detrend local antes da FFT\n",
        "\n",
        "# --- Configura√ß√µes da Feature \"Event Window\" ---\n",
        "EVENT_WINDOW_SIGMA_MULTIPLIER = 3.0 # Multiplicador de sigma para definir um pico (Roadmap R8.1)\n",
        "# Usa os mesmos WINDOW_SIZE_FFT e FFT_STEP para alinhar com features FFT\n",
        "\n",
        "# --- Configura√ß√µes do Modelo Preditivo (XGBoost) ---\n",
        "# Baseado no Roadmap R5.2\n",
        "XGB_PARAMS = {\n",
        "    'n_estimators': 300,\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.01,\n",
        "    'subsample': 0.6,\n",
        "    'colsample_bytree': 0.6,\n",
        "    'objective': 'reg:squarederror', # Para regress√£o\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1, # Usar todos os cores dispon√≠veis\n",
        "    'verbosity': 0 # xgb verbosity (0: silent, 1: warning, 2: info, 3: debug)\n",
        "}\n",
        "\n",
        "# --- Configura√ß√µes de Valida√ß√£o ---\n",
        "CV_N_SPLITS = 5 # N√∫mero de folds para TimeSeriesSplit (Roadmap R5.2)\n",
        "TEST_SET_SIZE = 0.1 # Propor√ß√£o dos dados para o conjunto de hold-out final (Roadmap R10.1)\n",
        "\n",
        "# --- Configura√ß√µes para Testes Estat√≠sticos de Compara√ß√£o de Modelos ---\n",
        "# Para Bootstrap CI na diferen√ßa de MAE (Roadmap R6.2, R7.2)\n",
        "BOOTSTRAP_N_REPLICAS = 1000\n",
        "BOOTSTRAP_CI_PERCENTILES = [2.5, 97.5]\n",
        "# N√≠vel de signific√¢ncia para testes estat√≠sticos (Wilcoxon, etc.)\n",
        "ALPHA_SIGNIFICANCE = 0.05\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# üî¨ FUN√á√ïES AUXILIARES\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "# === Bloco R1: Gera√ß√£o de Dados B√°sicos ===\n",
        "def generate_primes_gaps(n_primes_target: int, upper_bound_estimate: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Gera os primeiros `n_primes_target` n√∫meros primos e calcula os gaps entre eles.\n",
        "\n",
        "    Args:\n",
        "        n_primes_target: N√∫mero desejado de primos.\n",
        "        upper_bound_estimate: Uma estimativa superior para o n_primes_target-√©simo primo.\n",
        "\n",
        "    Returns:\n",
        "        Tuple contendo array de primos e array de gaps.\n",
        "    \"\"\"\n",
        "    print(f\"Gerando aproximadamente {n_primes_target:,} primos at√© o limite superior {upper_bound_estimate:,}...\")\n",
        "    try:\n",
        "        primes_list = list(primerange(2, upper_bound_estimate))\n",
        "    except TypeError: # Em algumas vers√µes antigas de sympy, pode haver problemas\n",
        "        print(\"Aviso: Houve um TypeError com sympy.primerange. Tentando converter limite para int.\")\n",
        "        primes_list = list(primerange(2, int(upper_bound_estimate)))\n",
        "\n",
        "\n",
        "    if len(primes_list) >= n_primes_target:\n",
        "        primes = np.array(primes_list[:n_primes_target], dtype=np.int64)\n",
        "    else:\n",
        "        print(f\"Aviso: A estimativa do limite superior ({upper_bound_estimate:,}) \"\n",
        "              f\"gerou apenas {len(primes_list):,} primos. \"\n",
        "              f\"Usando todos os primos gerados ou considere aumentar o limite.\")\n",
        "        primes = np.array(primes_list, dtype=np.int64)\n",
        "        if not primes.size:\n",
        "             raise ValueError(\"Nenhum primo gerado. Verifique o upper_bound_estimate.\")\n",
        "\n",
        "\n",
        "    gaps = np.diff(primes)\n",
        "    print(f\"Gerados {len(primes):,} primos, resultando em {len(gaps):,} gaps.\")\n",
        "    print(f\"√öltimo primo gerado: {primes[-1]:,}\")\n",
        "    print(f\"Estat√≠sticas b√°sicas dos gaps: M√©dia={np.mean(gaps):.2f}, Mediana={np.median(gaps):.2f}, StdDev={np.std(gaps):.2f}, \"\n",
        "          f\"Min={np.min(gaps)}, Max={np.max(gaps)}\")\n",
        "    return primes, gaps\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "rlDEe2tJPzb5",
        "outputId": "ea556129-bfc7-41a4-d637-ca06bce7c93a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fracdiff'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8c038b3a1eb7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetrend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstattools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madfuller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkpss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfracdiff\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFracdiff\u001b[0m \u001b[0;31m# Para diferencia√ß√£o fracion√°ria: pip install fracdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesSplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fracdiff'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Bloco R3: Estacionariedade e Diferencia√ß√£o (ARFIMA) ===\n",
        "def test_stationarity(timeseries: np.ndarray, series_name: str = \"S√©rie Temporal\"):\n",
        "    \"\"\"\n",
        "    Realiza testes de estacionariedade ADF e KPSS.\n",
        "\n",
        "    Args:\n",
        "        timeseries: S√©rie temporal como array numpy.\n",
        "        series_name: Nome da s√©rie para exibi√ß√£o.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Testes de Estacionariedade para: {series_name} ---\")\n",
        "    # Teste ADF (Augmented Dickey-Fuller)\n",
        "    # H0: A s√©rie possui raiz unit√°ria (n√£o √© estacion√°ria).\n",
        "    # p-value <= 0.05: Rejeita H0, s√©rie √© estacion√°ria.\n",
        "    adf_result = adfuller(timeseries)\n",
        "    print(f\"ADF Statistic: {adf_result[0]:.4f}\")\n",
        "    print(f\"ADF p-value: {adf_result[1]:.4f}\")\n",
        "    print(\"ADF Critical Values:\")\n",
        "    for key, value in adf_result[4].items():\n",
        "        print(f\"\\t{key}: {value:.4f}\")\n",
        "    if adf_result[1] <= ALPHA_SIGNIFICANCE:\n",
        "        print(\"Resultado ADF: S√©rie provavelmente estacion√°ria.\")\n",
        "    else:\n",
        "        print(\"Resultado ADF: S√©rie provavelmente n√£o estacion√°ria.\")\n",
        "\n",
        "    # Teste KPSS (Kwiatkowski-Phillips-Schmidt-Shin)\n",
        "    # H0: A s√©rie √© estacion√°ria em torno de uma tend√™ncia determin√≠stica (ou n√≠vel).\n",
        "    # p-value <= 0.05: Rejeita H0, s√©rie n√£o √© estacion√°ria.\n",
        "    # Usar 'c' para testar estacionariedade em torno de uma m√©dia constante.\n",
        "    # Usar 'ct' para testar estacionariedade em torno de uma tend√™ncia.\n",
        "    try:\n",
        "        kpss_result = kpss(timeseries, regression='c', nlags=\"auto\")\n",
        "        print(f\"\\nKPSS Statistic: {kpss_result[0]:.4f}\")\n",
        "        print(f\"KPSS p-value: {kpss_result[1]:.4f} (Aten√ß√£o: p-valor pode ser truncado em 0.01 ou 0.1)\")\n",
        "        print(\"KPSS Critical Values:\")\n",
        "        for key, value in kpss_result[3].items():\n",
        "            print(f\"\\t{key}: {value:.4f}\")\n",
        "        if kpss_result[1] > ALPHA_SIGNIFICANCE: # Se p-valor > alpha, n√£o rejeita H0\n",
        "            print(\"Resultado KPSS: S√©rie provavelmente estacion√°ria.\")\n",
        "        else:\n",
        "            print(\"Resultado KPSS: S√©rie provavelmente n√£o estacion√°ria.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao executar KPSS: {e}. Pode ser devido a poucos dados ou vari√¢ncia zero.\")\n",
        "\n",
        "\n",
        "def difference_series(series: np.ndarray, order: int = 1) -> np.ndarray:\n",
        "    \"\"\"Aplica diferencia√ß√£o inteira a uma s√©rie.\"\"\"\n",
        "    if order < 1:\n",
        "        return series\n",
        "    return np.diff(series, n=order)\n",
        "\n",
        "def fractional_difference_series(series: pd.Series, d: float, threshold: float = 1e-4) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Aplica diferencia√ß√£o fracion√°ria usando a biblioteca fracdiff.\n",
        "\n",
        "    Args:\n",
        "        series: S√©rie temporal como pandas Series.\n",
        "        d: Ordem da diferencia√ß√£o fracion√°ria.\n",
        "        threshold: Limiar para os pesos da diferencia√ß√£o.\n",
        "                   Os pesos w s√£o calculados at√© que abs(w) < threshold.\n",
        "                   Um valor menor significa mais termos, potencialmente mais preciso\n",
        "                   mas mais lento e pode requerer mais dados hist√≥ricos.\n",
        "                   O default da lib `fracdiff` √© 0.0001.\n",
        "\n",
        "    Returns:\n",
        "        S√©rie diferenciada fracionariamente como pandas Series.\n",
        "        Pode conter NaNs no in√≠cio.\n",
        "    \"\"\"\n",
        "    print(f\"Aplicando diferencia√ß√£o fracion√°ria com d={d:.3f} e threshold={threshold}...\")\n",
        "    # Fracdiff espera um DataFrame ou Series. Retorna um DataFrame.\n",
        "    fdiff_model = Fracdiff(d, threshold=threshold, window=len(series)) # window=len(series) para usar todos os dados dispon√≠veis para c√°lculo\n",
        "    transformed_series = fdiff_model.fit_transform(series.to_frame())[series.name]\n",
        "    return transformed_series.dropna()\n"
      ],
      "metadata": {
        "id": "g-Sxd5abP-bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni514wwZPSGz"
      },
      "outputs": [],
      "source": [
        "# === Bloco R2 & R4 & R8: Engenharia de Features ===\n",
        "def get_rolling_stats_features(gaps_data: np.ndarray, window_size: int, step: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calcula features de estat√≠sticas robustas em janela deslizante (Median, MAD)\n",
        "    e o pr√≥ximo gap (target).\n",
        "\n",
        "    Args:\n",
        "        gaps_data: Array dos gaps.\n",
        "        window_size: Tamanho da janela deslizante.\n",
        "        step: Passo da janela deslizante.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame com Median_Gap, MAD_Gap, e Next_Gap.\n",
        "    \"\"\"\n",
        "    print(f\"Calculando features de estat√≠sticas robustas (Janela={window_size}, Passo={step})...\")\n",
        "    features = []\n",
        "    num_windows = (len(gaps_data) - window_size - 1) // step + 1 # -1 para ter um Next_Gap\n",
        "\n",
        "    for i in range(num_windows):\n",
        "        start_idx = i * step\n",
        "        end_idx = start_idx + window_size\n",
        "        window = gaps_data[start_idx:end_idx]\n",
        "\n",
        "        median_gap = np.median(window)\n",
        "        # median_abs_deviation requer `scale='normal'` para consist√™ncia com std em dados normais\n",
        "        # ou `scale=1` para MAD puro. Usaremos MAD puro.\n",
        "        mad_gap = median_abs_deviation(window, scale=1)\n",
        "\n",
        "        next_gap = gaps_data[end_idx] # O gap imediatamente ap√≥s a janela atual\n",
        "\n",
        "        features.append({\n",
        "            'Median_Gap': median_gap,\n",
        "            'MAD_Gap': mad_gap,\n",
        "            'Next_Gap': next_gap,\n",
        "            'Original_Index': end_idx # Guarda o √≠ndice original do Next_Gap para alinhamento futuro\n",
        "        })\n",
        "\n",
        "    df_feats = pd.DataFrame(features)\n",
        "    if df_feats.empty:\n",
        "        raise ValueError(\"Nenhuma feature de estat√≠sticas robustas foi gerada. Verifique o tamanho dos dados e da janela.\")\n",
        "    return df_feats.set_index('Original_Index', drop=True)\n",
        "\n",
        "\n",
        "def get_fft_features(gaps_data: np.ndarray, window_size: int, step: int,\n",
        "                     top_k: int, fft_window_type: str, apply_detrend: bool) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extrai features de FFT avan√ßadas de janelas deslizantes de gaps.\n",
        "    Inclui detrend local, janelamento (Hanning/Hamming), zero-padding,\n",
        "    e extra√ß√£o de magnitude normalizada, energia relativa e energia absoluta.\n",
        "\n",
        "    Args:\n",
        "        gaps_data: Array dos gaps.\n",
        "        window_size: Tamanho da janela para FFT.\n",
        "        step: Passo da janela deslizante.\n",
        "        top_k: N√∫mero das 'k' maiores frequ√™ncias/magnitudes a serem extra√≠das.\n",
        "        fft_window_type: Tipo de janela para aplicar (ex: 'hann', 'hamming').\n",
        "        apply_detrend: Booleano, se True aplica detrend linear local.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame com as features de FFT.\n",
        "    \"\"\"\n",
        "    print(f\"Calculando features de FFT (Janela={window_size}, Passo={step}, TopK={top_k})...\")\n",
        "    features_fft_list = []\n",
        "\n",
        "    # Prepara as janelas de forma vetorizada (stride tricks)\n",
        "    n_gaps = len(gaps_data)\n",
        "    num_windows = (n_gaps - window_size) // step + 1\n",
        "\n",
        "    # shape e strides para criar views das janelas sem copiar dados\n",
        "    shape = (num_windows, window_size)\n",
        "    strides = (gaps_data.strides[0] * step, gaps_data.strides[0])\n",
        "    windows_data = np.lib.stride_tricks.as_strided(gaps_data, shape=shape, strides=strides)\n",
        "\n",
        "    if apply_detrend:\n",
        "        windows_data = detrend(windows_data, axis=1, type='linear')\n",
        "\n",
        "    # Aplica janela de Hanning/Hamming etc.\n",
        "    window_func = get_window(fft_window_type, window_size)\n",
        "    windows_data = windows_data * window_func\n",
        "\n",
        "    # Zero-padding para a pr√≥xima pot√™ncia de 2 para melhor resolu√ß√£o de frequ√™ncia\n",
        "    nfft = 1 << (window_size - 1).bit_length()\n",
        "\n",
        "    # Calcula FFT real para todas as janelas de uma vez\n",
        "    fft_results = np.fft.rfft(windows_data, n=nfft, axis=1)\n",
        "    magnitudes = np.abs(fft_results)\n",
        "    frequencies = np.fft.rfftfreq(nfft, d=1.0) # d=1.0 pois os gaps s√£o sequenciais\n",
        "\n",
        "    # Calcula energia absoluta por janela (sobre os dados janelados e possivelmente detrended)\n",
        "    energy_abs_per_window = np.sum(windows_data**2, axis=1)\n",
        "\n",
        "    # Processa cada janela para extrair as top_k features\n",
        "    for i in range(num_windows):\n",
        "        mag_window = magnitudes[i, :]\n",
        "\n",
        "        # Ignora a componente DC (frequ√™ncia zero) para picos\n",
        "        mag_window_no_dc = mag_window[1:]\n",
        "        freq_no_dc = frequencies[1:]\n",
        "\n",
        "        # √çndices das top_k maiores magnitudes (descontando DC)\n",
        "        # Adiciona 1 de volta aos √≠ndices para alinhar com `magnitudes` e `frequencies` originais\n",
        "        # se `freq_no_dc` foi usado para `argsort`.\n",
        "        # Se `mag_window_no_dc` √© vazio (ex: window_size=1), argsort falha.\n",
        "        if mag_window_no_dc.size > 0:\n",
        "            idxs_top_k = np.argsort(mag_window_no_dc)[-top_k:][::-1] + 1\n",
        "        else: # Caso raro, janela muito pequena\n",
        "            idxs_top_k = np.array([], dtype=int)\n",
        "\n",
        "        current_features = {}\n",
        "        sum_mag_total_window = np.sum(mag_window[1:]) # Soma das magnitudes (excluindo DC)\n",
        "        sum_energy_total_window = np.sum(mag_window[1:]**2) # Soma das energias (excluindo DC)\n",
        "\n",
        "        for rank, idx_freq in enumerate(idxs_top_k, start=1):\n",
        "            freq_val = frequencies[idx_freq]\n",
        "            mag_val = magnitudes[i, idx_freq]\n",
        "\n",
        "            current_features[f'FFT{rank}_freq'] = freq_val\n",
        "            current_features[f'FFT{rank}_mag_norm'] = mag_val / sum_mag_total_window if sum_mag_total_window > 0 else 0.0\n",
        "            current_features[f'FFT{rank}_energy_rel'] = (mag_val**2) / sum_energy_total_window if sum_energy_total_window > 0 else 0.0\n",
        "\n",
        "        current_features['FFT_Energy_Abs'] = energy_abs_per_window[i]\n",
        "\n",
        "        # Adiciona Original_Index para alinhamento\n",
        "        # A feature FFT descreve a janela que *antecede* o `Next_Gap`.\n",
        "        # Se uma janela √© gaps[k:k+W], o Next_Gap que queremos prever √© gaps[k+W].\n",
        "        # O √≠ndice original aqui se refere ao final da janela.\n",
        "        original_idx_of_window_end = (i * step) + window_size\n",
        "        current_features['Original_Index'] = original_idx_of_window_end\n",
        "\n",
        "        features_fft_list.append(current_features)\n",
        "\n",
        "    df_fft = pd.DataFrame(features_fft_list)\n",
        "    if df_fft.empty:\n",
        "        # N√£o levantar erro, pode ser que n√£o haja janelas suficientes\n",
        "        print(\"Aviso: Nenhuma feature de FFT foi gerada. Verifique o tamanho dos dados e da janela.\")\n",
        "        # Retornar DataFrame vazio com colunas esperadas para evitar quebras downstream\n",
        "        cols = [f'FFT{r+1}_{s}' for r in range(top_k) for s in ['freq', 'mag_norm', 'energy_rel']] + ['FFT_Energy_Abs', 'Original_Index']\n",
        "        return pd.DataFrame(columns=cols).set_index('Original_Index', drop=True)\n",
        "\n",
        "    return df_fft.set_index('Original_Index', drop=True)\n",
        "\n",
        "\n",
        "def get_event_window_feature(gaps_data: np.ndarray, window_size: int, step: int,\n",
        "                             sigma_multiplier: float) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cria uma feature bin√°ria 'Event_Window' que √© 1 se qualquer gap\n",
        "    dentro da janela exceder k*sigma_global, 0 caso contr√°rio.\n",
        "\n",
        "    Args:\n",
        "        gaps_data: Array dos gaps (a s√©rie completa para calcular sigma_global).\n",
        "        window_size: Tamanho da janela.\n",
        "        step: Passo da janela.\n",
        "        sigma_multiplier: Multiplicador k para definir o threshold (k * sigma_global).\n",
        "\n",
        "    Returns:\n",
        "        DataFrame com a feature 'Event_Window'.\n",
        "    \"\"\"\n",
        "    print(f\"Calculando feature 'Event Window' (Sigma_Multiplier={sigma_multiplier})...\")\n",
        "    if len(gaps_data) < window_size : # N√£o h√° janelas suficientes\n",
        "        print(\"Aviso: N√£o h√° dados suficientes para gerar features 'Event Window'.\")\n",
        "        return pd.DataFrame(columns=['Event_Window', 'Original_Index']).set_index('Original_Index', drop=True)\n",
        "\n",
        "    sigma_global = np.std(gaps_data)\n",
        "    threshold = sigma_multiplier * sigma_global\n",
        "\n",
        "    event_flags = []\n",
        "\n",
        "    # Prepara as janelas de forma vetorizada (stride tricks)\n",
        "    n_gaps = len(gaps_data)\n",
        "    num_windows = (n_gaps - window_size) // step + 1 # Garante que a janela caiba nos dados\n",
        "\n",
        "    shape = (num_windows, window_size)\n",
        "    strides = (gaps_data.strides[0] * step, gaps_data.strides[0])\n",
        "    windows_data = np.lib.stride_tricks.as_strided(gaps_data, shape=shape, strides=strides)\n",
        "\n",
        "    # Verifica se o m√°ximo em cada janela excede o threshold\n",
        "    max_in_window = np.max(windows_data, axis=1)\n",
        "    event_flags_vector = (max_in_window > threshold).astype(int)\n",
        "\n",
        "    original_indices = [(i * step) + window_size for i in range(num_windows)]\n",
        "\n",
        "    df_event = pd.DataFrame({\n",
        "        'Event_Window': event_flags_vector,\n",
        "        'Original_Index': original_indices\n",
        "    })\n",
        "    return df_event.set_index('Original_Index', drop=True)\n",
        "\n",
        "\n",
        "# === Bloco R5, R9, R10: Modelagem e Valida√ß√£o ===\n",
        "def train_evaluate_model(X_train: pd.DataFrame, y_train: pd.Series,\n",
        "                         X_test: pd.DataFrame, y_test: pd.Series,\n",
        "                         model_params: dict, model_name:str = \"XGBoost\"):\n",
        "    \"\"\"\n",
        "    Treina um modelo XGBoost e o avalia no conjunto de teste.\n",
        "    \"\"\"\n",
        "    print(f\"\\nTreinando modelo {model_name}...\")\n",
        "    model = xgb.XGBRegressor(**model_params)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Avaliando modelo {model_name} no conjunto de teste...\")\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "\n",
        "    print(f\"Performance de {model_name} no Teste: MAE = {mae:.3f}, RMSE = {rmse:.3f}\")\n",
        "    return model, mae, rmse, predictions\n",
        "\n",
        "\n",
        "def cross_validate_model(X: pd.DataFrame, y: pd.Series, model_params: dict,\n",
        "                         n_splits: int, model_name:str = \"XGBoost\"):\n",
        "    \"\"\"\n",
        "    Realiza valida√ß√£o cruzada temporal para o modelo XGBoost.\n",
        "    \"\"\"\n",
        "    print(f\"\\nIniciando Valida√ß√£o Cruzada Temporal para {model_name} ({n_splits} folds)...\")\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    maes_fold, rmses_fold = [], []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
        "        print(f\"  Fold {fold+1}/{n_splits}\")\n",
        "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # (Opcional) Escalonamento de features dentro do fold do CV\n",
        "        # scaler = StandardScaler()\n",
        "        # X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
        "        # X_val_fold_scaled = scaler.transform(X_val_fold)\n",
        "        # model_fold, mae, rmse, _ = train_evaluate_model(\n",
        "        #     pd.DataFrame(X_train_fold_scaled, columns=X_train_fold.columns, index=X_train_fold.index), y_train_fold,\n",
        "        #     pd.DataFrame(X_val_fold_scaled, columns=X_val_fold.columns, index=X_val_fold.index), y_val_fold,\n",
        "        #     model_params, model_name=f\"{model_name} Fold {fold+1}\"\n",
        "        # )\n",
        "\n",
        "        # Sem escalonamento por enquanto, para manter simples. Adicionar se necess√°rio.\n",
        "        _, mae, rmse, _ = train_evaluate_model(\n",
        "            X_train_fold, y_train_fold,\n",
        "            X_val_fold, y_val_fold,\n",
        "            model_params, model_name=f\"{model_name} Fold {fold+1} (CV)\"\n",
        "        )\n",
        "        maes_fold.append(mae)\n",
        "        rmses_fold.append(rmse)\n",
        "\n",
        "    print(f\"\\nResultados da Valida√ß√£o Cruzada Temporal para {model_name}:\")\n",
        "    print(f\"  MAE m√©dio: {np.mean(maes_fold):.3f} ¬± {np.std(maes_fold):.3f}\")\n",
        "    print(f\"  RMSE m√©dio: {np.mean(rmses_fold):.3f} ¬± {np.std(rmses_fold):.3f}\")\n",
        "    return maes_fold, rmses_fold\n",
        "\n",
        "# === Bloco R6, R7: Teste Estat√≠stico da Redu√ß√£o de Erro ===\n",
        "def compare_model_performance_statistically(errors_model1: list, errors_model2: list,\n",
        "                                            model1_name: str = \"Modelo 1\",\n",
        "                                            model2_name: str = \"Modelo 2 (Melhorado)\",\n",
        "                                            metric_name: str = \"MAE\"):\n",
        "    \"\"\"\n",
        "    Compara a performance de dois modelos usando Wilcoxon signed-rank test\n",
        "    e Bootstrap CI na diferen√ßa das m√©tricas de erro (ex: MAE por fold).\n",
        "    Assume que `errors_model1` s√£o os erros do modelo baseline e\n",
        "    `errors_model2` s√£o os erros do modelo que se espera ser melhor (menor erro).\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Compara√ß√£o Estat√≠stica de Performance: {model1_name} vs {model2_name} ({metric_name}) ---\")\n",
        "    errors_model1 = np.array(errors_model1)\n",
        "    errors_model2 = np.array(errors_model2)\n",
        "\n",
        "    if len(errors_model1) != len(errors_model2) or len(errors_model1) < 2:\n",
        "        print(\"Erro: As listas de erro devem ter o mesmo tamanho e pelo menos 2 elementos para compara√ß√£o.\")\n",
        "        return\n",
        "\n",
        "    differences = errors_model1 - errors_model2 # Positivo se modelo 2 √© melhor (menor erro)\n",
        "\n",
        "    mean_diff = np.mean(differences)\n",
        "    std_diff = np.std(differences, ddof=1)\n",
        "    print(f\"Diferen√ßa m√©dia ({metric_name} {model1_name} - {metric_name} {model2_name}): {mean_diff:.4f} ¬± {std_diff:.4f}\")\n",
        "\n",
        "    # Wilcoxon signed-rank test\n",
        "    # H0: A mediana das diferen√ßas √© zero.\n",
        "    # H1: A mediana das diferen√ßas n√£o √© zero (ou √© > 0 se 'alternative' for 'greater').\n",
        "    # Usamos 'greater' para testar se model1_errors > model2_errors (i.e., diffs > 0)\n",
        "    try:\n",
        "        # O teste requer que n√£o haja apenas zeros nas diferen√ßas, e n√£o sejam todos iguais.\n",
        "        if np.all(differences == differences[0]): # Todos os diffs s√£o iguais\n",
        "             print(\"Wilcoxon: Todas as diferen√ßas s√£o id√™nticas. Teste n√£o aplic√°vel ou informativo.\")\n",
        "             stat_w, p_w = np.nan, np.nan\n",
        "        elif np.all(differences == 0): # Todos os diffs s√£o zero\n",
        "            print(\"Wilcoxon: Todas as diferen√ßas s√£o zero. Modelos id√™nticos em performance.\")\n",
        "            stat_w, p_w = np.nan, np.nan\n",
        "        else:\n",
        "            stat_w, p_w = wilcoxon(differences, alternative='greater')\n",
        "            print(f\"Wilcoxon signed-rank test (H1: {model2_name} √© melhor): W = {stat_w:.3f}, p-value = {p_w:.4f}\")\n",
        "            if p_w <= ALPHA_SIGNIFICANCE:\n",
        "                print(f\"  Resultado: Rejeita H0. Evid√™ncia estat√≠stica de que {model2_name} tem {metric_name} significativamente menor.\")\n",
        "            else:\n",
        "                print(f\"  Resultado: N√£o rejeita H0. N√£o h√° evid√™ncia estat√≠stica suficiente de que {model2_name} √© melhor.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Erro no teste de Wilcoxon: {e}. Pode ser devido a poucas amostras ou todas as diferen√ßas serem zero.\")\n",
        "        stat_w, p_w = np.nan, np.nan\n",
        "\n",
        "\n",
        "    # Bootstrap CI para a m√©dia da diferen√ßa\n",
        "    boot_means_diff = []\n",
        "    for _ in range(BOOTSTRAP_N_REPLICAS):\n",
        "        sample_diffs = resample(differences, replace=True, n_samples=len(differences))\n",
        "        boot_means_diff.append(np.mean(sample_diffs))\n",
        "\n",
        "    ci_lower, ci_upper = np.percentile(boot_means_diff, BOOTSTRAP_CI_PERCENTILES)\n",
        "    print(f\"Bootstrap {100 - 2*BOOTSTRAP_CI_PERCENTILES[0]:.0f}% CI para a m√©dia da diferen√ßa [{metric_name} redu√ß√£o]: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
        "    if ci_lower > 0:\n",
        "        print(f\"  Resultado CI: Intervalo de confian√ßa est√° acima de zero, sugerindo que {model2_name} √© melhor.\")\n",
        "    elif ci_upper < 0:\n",
        "         print(f\"  Resultado CI: Intervalo de confian√ßa est√° abaixo de zero, sugerindo que {model1_name} √© melhor.\")\n",
        "    else:\n",
        "        print(f\"  Resultado CI: Intervalo de confian√ßa cont√©m zero, n√£o conclusivo sobre qual modelo √© melhor.\")\n",
        "\n",
        "\n",
        "# === Bloco R11: Permutation Importance ===\n",
        "def get_permutation_feature_importance(model, X_val: pd.DataFrame, y_val: pd.Series,\n",
        "                                       n_repeats: int = 10):\n",
        "    \"\"\"\n",
        "    Calcula a import√¢ncia das features por permuta√ß√£o.\n",
        "    \"\"\"\n",
        "    print(f\"\\nCalculando Permutation Feature Importance (n_repeats={n_repeats})...\")\n",
        "    perm_importance = permutation_importance(model, X_val, y_val, n_repeats=n_repeats, random_state=XGB_PARAMS['random_state'])\n",
        "\n",
        "    sorted_idx = perm_importance.importances_mean.argsort()[::-1]\n",
        "\n",
        "    print(\"Import√¢ncia das Features por Permuta√ß√£o (queda m√©dia no score):\")\n",
        "    for i in sorted_idx:\n",
        "        print(f\"  {X_val.columns[i]:<30}: {perm_importance.importances_mean[i]:.4f} ¬± {perm_importance.importances_std[i]:.4f}\")\n",
        "\n",
        "    df_perm_importance = pd.DataFrame({\n",
        "        'feature': X_val.columns[sorted_idx],\n",
        "        'importance_mean': perm_importance.importances_mean[sorted_idx],\n",
        "        'importance_std': perm_importance.importances_std[sorted_idx]\n",
        "    })\n",
        "    return df_perm_importance\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# üöÄ FLUXO PRINCIPAL DE AN√ÅLISE (WORKFLOW)\n",
        "# --------------------------------------------------------------------------\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Fun√ß√£o principal para executar o pipeline de an√°lise.\n",
        "    \"\"\"\n",
        "    print(\"--- Iniciando Pipeline de An√°lise de Gaps de Primos ---\")\n",
        "\n",
        "    # --- 1. Gera√ß√£o de Dados (R1) ---\n",
        "    primes, gaps_raw = generate_primes_gaps(N_PRIMES, PRIME_UPPER_BOUND_ESTIMATE)\n",
        "\n",
        "    # --- 2. An√°lise de Estacionariedade e Diferencia√ß√£o (R3 & ARFIMA) ---\n",
        "    test_stationarity(gaps_raw, \"Gaps Brutos\")\n",
        "\n",
        "    gaps_processed = gaps_raw.copy() # Come√ßa com os gaps brutos\n",
        "    processed_series_name = \"Gaps Brutos\"\n",
        "\n",
        "    if APPLY_DIFFERENCING:\n",
        "        if DIFFERENCING_TYPE == 'integer':\n",
        "            print(f\"\\nAplicando diferencia√ß√£o inteira de ordem {INTEGER_DIFF_ORDER}...\")\n",
        "            gaps_processed = difference_series(gaps_raw, order=INTEGER_DIFF_ORDER)\n",
        "            processed_series_name = f\"Gaps Diferenciados (d={INTEGER_DIFF_ORDER})\"\n",
        "        elif DIFFERENCING_TYPE == 'fractional':\n",
        "            # Diferencia√ß√£o fracion√°ria requer pandas Series e remove NaNs,\n",
        "            # o que encurtar√° a s√©rie. Precisamos lidar com isso no alinhamento.\n",
        "            print(f\"\\nAplicando diferencia√ß√£o fracion√°ria com d={FRACTIONAL_DIFF_D}...\")\n",
        "            # Guardar o √≠ndice original antes de converter para Series para `fracdiff`\n",
        "            # e depois alinhar. Fracdiff n√£o preserva o √≠ndice original diretamente.\n",
        "            # No entanto, as features s√£o em janela, ent√£o o importante √© o `gaps_processed`\n",
        "            # ter o comprimento correto para a gera√ß√£o de features.\n",
        "\n",
        "            # Para `fracdiff`, precisamos de uma s√©rie pandas\n",
        "            gaps_series_for_fd = pd.Series(gaps_raw, name=\"gaps\")\n",
        "            gaps_frac_diff = fractional_difference_series(gaps_series_for_fd, d=FRACTIONAL_DIFF_D)\n",
        "\n",
        "            # As features ser√£o calculadas sobre esta s√©rie diferenciada.\n",
        "            # Precisamos manter o controle de quantos pontos perdemos no in√≠cio devido √† diferencia√ß√£o.\n",
        "            num_nans_from_fd = len(gaps_raw) - len(gaps_frac_diff)\n",
        "            print(f\"N√∫mero de pontos perdidos no in√≠cio devido √† diferencia√ß√£o fracion√°ria: {num_nans_from_fd}\")\n",
        "            gaps_processed = gaps_frac_diff.values # Converte de volta para numpy array\n",
        "            processed_series_name = f\"Gaps Diferenciados Frac. (d={FRACTIONAL_DIFF_D:.3f})\"\n",
        "\n",
        "        # Testa estacionariedade da s√©rie processada\n",
        "        if len(gaps_processed) > 10 : # Teste requer alguns pontos\n",
        "             test_stationarity(gaps_processed, processed_series_name)\n",
        "        else:\n",
        "            print(f\"Aviso: S√©rie processada '{processed_series_name}' √© muito curta para teste de estacionariedade.\")\n",
        "\n",
        "\n",
        "    # --- 3. Engenharia de Features (R2, R4, R8) ---\n",
        "    # Todas as features (rolling stats, fft, event window) devem ser calculadas\n",
        "    # sobre `gaps_processed`. O `Next_Gap` (target) tamb√©m ser√° derivado de `gaps_processed`.\n",
        "\n",
        "    # 3a. Features de Estat√≠sticas Robustas\n",
        "    # O `target_df` j√° cont√©m `Next_Gap` e `Original_Index` (do final da janela que o gerou)\n",
        "    # Esta fun√ß√£o agora precisa lidar com o `gaps_processed`\n",
        "    if len(gaps_processed) < WINDOW_SIZE_STATS + 1:\n",
        "        raise ValueError(f\"S√©rie '{processed_series_name}' muito curta ({len(gaps_processed)} pontos) para gerar features de estat√≠sticas com janela {WINDOW_SIZE_STATS}.\")\n",
        "\n",
        "    df_stats_feats = get_rolling_stats_features(gaps_processed,\n",
        "                                                window_size=WINDOW_SIZE_STATS,\n",
        "                                                step=STATS_STEP)\n",
        "    y_target = df_stats_feats['Next_Gap']\n",
        "    df_stats_feats_for_X = df_stats_feats.drop(columns=['Next_Gap'])\n",
        "\n",
        "\n",
        "    # 3b. Features de FFT\n",
        "    if len(gaps_processed) >= WINDOW_SIZE_FFT:\n",
        "        df_fft_feats = get_fft_features(gaps_processed,\n",
        "                                        window_size=WINDOW_SIZE_FFT,\n",
        "                                        step=FFT_STEP,\n",
        "                                        top_k=FFT_TOP_K,\n",
        "                                        fft_window_type=FFT_WINDOW_TYPE,\n",
        "                                        apply_detrend=FFT_APPLY_DETREND)\n",
        "    else:\n",
        "        print(f\"Aviso: S√©rie '{processed_series_name}' ({len(gaps_processed)} pontos) muito curta para FFT com janela {WINDOW_SIZE_FFT}. Pulando features FFT.\")\n",
        "        df_fft_feats = pd.DataFrame(index=df_stats_feats_for_X.index) # DataFrame vazio com mesmo √≠ndice\n",
        "\n",
        "    # 3c. Feature \"Event Window\"\n",
        "    # Usa WINDOW_SIZE_FFT e FFT_STEP para alinhar com as janelas FFT\n",
        "    if len(gaps_processed) >= WINDOW_SIZE_FFT: # Mesmo requisito de tamanho de FFT\n",
        "        df_event_feat = get_event_window_feature(gaps_processed,\n",
        "                                                 window_size=WINDOW_SIZE_FFT, # Alinhado com FFT\n",
        "                                                 step=FFT_STEP, # Alinhado com FFT\n",
        "                                                 sigma_multiplier=EVENT_WINDOW_SIGMA_MULTIPLIER)\n",
        "    else:\n",
        "        print(f\"Aviso: S√©rie '{processed_series_name}' ({len(gaps_processed)} pontos) muito curta para Event Window com janela {WINDOW_SIZE_FFT}. Pulando feature.\")\n",
        "        df_event_feat = pd.DataFrame(index=df_stats_feats_for_X.index) # DataFrame vazio\n",
        "\n",
        "\n",
        "    # 3d. Montar DataFrame final de features (X)\n",
        "    # Alinhamento √© crucial aqui. Todas as DFs de features usam 'Original_Index'.\n",
        "    # O `y_target` j√° est√° alinhado com `df_stats_feats_for_X` pelo √≠ndice.\n",
        "\n",
        "    # Come√ßa com as features de estat√≠sticas\n",
        "    X_all_features = df_stats_feats_for_X.copy()\n",
        "\n",
        "    # Merge features FFT (se existirem)\n",
        "    if not df_fft_feats.empty:\n",
        "        X_all_features = X_all_features.join(df_fft_feats, how='left')\n",
        "\n",
        "    # Merge feature Event Window (se existir)\n",
        "    if not df_event_feat.empty:\n",
        "        X_all_features = X_all_features.join(df_event_feat, how='left')\n",
        "\n",
        "    # Assegurar que y_target (Next_Gap) est√° alinhado com o √≠ndice final de X_all_features\n",
        "    # Isso √© importante porque diferentes features (stats, fft) podem ter diferentes `step`\n",
        "    # e, portanto, diferentes n√∫meros de janelas.\n",
        "    # A abordagem aqui √© que df_stats_feats (com step=1 por default) define o `y_target` e o √≠ndice mestre.\n",
        "    # Outras features (FFT, Event) com steps maiores ser√£o esparsas (NaNs) se juntadas com 'left'.\n",
        "    # Se os steps s√£o diferentes, precisamos de uma estrat√©gia de alinhamento.\n",
        "    # Assumindo por agora que o `Original_Index` de todas as features √© consistente\n",
        "    # para os pontos onde s√£o calculados.\n",
        "    # Se FFT_STEP != STATS_STEP, precisaremos preencher NaNs ou usar um step comum.\n",
        "    # Para simplificar, vamos assumir STATS_STEP=1 e outros steps podem ser maiores.\n",
        "    # As colunas de features com steps maiores ter√£o NaNs, que precisam ser tratados (ex: ffill, ou dropna).\n",
        "\n",
        "    # Neste ponto, X_all_features e y_target compartilham o mesmo √≠ndice 'Original_Index'.\n",
        "    # Vamos remover quaisquer linhas onde o y_target possa ser NaN (n√£o deve acontecer com a l√≥gica atual)\n",
        "    # ou onde features essenciais s√£o NaN.\n",
        "\n",
        "    print(f\"\\nForma de X_all_features antes de tratar NaNs: {X_all_features.shape}\")\n",
        "    print(f\"Colunas em X_all_features: {X_all_features.columns.tolist()}\")\n",
        "\n",
        "    # Tratar NaNs que podem surgir do join se os steps forem diferentes\n",
        "    # Uma estrat√©gia simples √© preencher para frente (ffill) e depois para tr√°s (bfill)\n",
        "    # Isso assume que as features de FFT/Event s√£o \"v√°lidas\" para as janelas intermedi√°rias de stats.\n",
        "    # CUIDADO: Esta √© uma simplifica√ß√£o. Se FFT_STEP > STATS_STEP, usar ffill significa que\n",
        "    # m√∫ltiplas janelas de stats (com step=1) usar√£o a mesma feature FFT da janela FFT anterior.\n",
        "    if FFT_STEP != STATS_STEP or (EVENT_WINDOW_SIGMA_MULTIPLIER and FFT_STEP != STATS_STEP): # Se event window usa FFT_STEP\n",
        "        print(f\"Aplicando ffill e bfill para NaNs devido a diferentes steps de features...\")\n",
        "        X_all_features = X_all_features.ffill().bfill()\n",
        "\n",
        "    # Remove linhas onde qualquer feature ainda √© NaN (ex: no in√≠cio se bfill n√£o cobrir)\n",
        "    # E tamb√©m garante que y_target esteja alinhado e n√£o seja NaN.\n",
        "    common_index = X_all_features.index.intersection(y_target.index)\n",
        "    X_all_features = X_all_features.loc[common_index]\n",
        "    y = y_target.loc[common_index]\n",
        "\n",
        "    X_all_features = X_all_features.dropna()\n",
        "    y = y.loc[X_all_features.index] # Realinhar y ap√≥s dropar NaNs de X\n",
        "\n",
        "    if X_all_features.empty or y.empty:\n",
        "        raise ValueError(\"Nenhum dado de feature/target utiliz√°vel ap√≥s tratamento de NaNs. Verifique os steps e tamanhos de janela.\")\n",
        "\n",
        "    print(f\"Forma final de X: {X_all_features.shape}, Forma final de y: {y.shape}\")\n",
        "\n",
        "\n",
        "    # --- 4. Separa√ß√£o de Treino/CV e Hold-out (R10) ---\n",
        "    # Antes de qualquer CV intensivo, separamos o hold-out.\n",
        "    # `TimeSeriesSplit` n√£o √© usado para esta divis√£o inicial.\n",
        "    hold_out_idx_start = math.floor(len(X_all_features) * (1 - TEST_SET_SIZE))\n",
        "\n",
        "    X_train_cv = X_all_features.iloc[:hold_out_idx_start]\n",
        "    y_train_cv = y.iloc[:hold_out_idx_start]\n",
        "    X_holdout = X_all_features.iloc[hold_out_idx_start:]\n",
        "    y_holdout = y.iloc[hold_out_idx_start:]\n",
        "\n",
        "    print(f\"\\nDados divididos em:\")\n",
        "    print(f\"  Treino/CV: X_train_cv shape={X_train_cv.shape}, y_train_cv shape={y_train_cv.shape}\")\n",
        "    print(f\"  Hold-out:  X_holdout shape={X_holdout.shape}, y_holdout shape={y_holdout.shape}\")\n",
        "\n",
        "    if X_train_cv.empty or X_holdout.empty:\n",
        "        raise ValueError(\"Conjunto de treino/CV ou hold-out est√° vazio. Ajuste TEST_SET_SIZE ou verifique os dados.\")\n",
        "\n",
        "    # --- 5. Valida√ß√£o Preditiva e Compara√ß√£o de Modelos ---\n",
        "    # 5a. Modelo Baseline (apenas Median_Gap, MAD_Gap)\n",
        "    baseline_features = ['Median_Gap', 'MAD_Gap']\n",
        "    # Verificar se as colunas existem, caso contr√°rio pular baseline\n",
        "    if all(feat in X_train_cv.columns for feat in baseline_features):\n",
        "        X_baseline_train_cv = X_train_cv[baseline_features]\n",
        "        maes_baseline_cv, _ = cross_validate_model(X_baseline_train_cv, y_train_cv,\n",
        "                                                   XGB_PARAMS, CV_N_SPLITS,\n",
        "                                                   model_name=\"Baseline (Stats Only)\")\n",
        "    else:\n",
        "        print(\"Aviso: Colunas para modelo baseline n√£o encontradas. Pulando modelo baseline.\")\n",
        "        maes_baseline_cv = None\n",
        "\n",
        "    # 5b. Modelo Completo (com todas as features selecionadas)\n",
        "    maes_full_model_cv, _ = cross_validate_model(X_train_cv, y_train_cv,\n",
        "                                                 XGB_PARAMS, CV_N_SPLITS,\n",
        "                                                 model_name=\"Full Model (All Features)\")\n",
        "\n",
        "    # --- 6. Teste Estat√≠stico da Redu√ß√£o de Erro (R6) ---\n",
        "    if maes_baseline_cv and maes_full_model_cv:\n",
        "        compare_model_performance_statistically(maes_baseline_cv, maes_full_model_cv,\n",
        "                                                model1_name=\"Baseline Model\",\n",
        "                                                model2_name=\"Full Model\",\n",
        "                                                metric_name=\"MAE (CV)\")\n",
        "\n",
        "    # --- 7. Treinamento do Modelo Final e Avalia√ß√£o no Hold-out (R10) ---\n",
        "    print(\"\\n--- Treinando Modelo Final em todo o conjunto de Treino/CV ---\")\n",
        "    # Opcional: escalar X_train_cv antes de treinar modelo final\n",
        "    # scaler_final = StandardScaler().fit(X_train_cv)\n",
        "    # X_train_cv_scaled = scaler_final.transform(X_train_cv)\n",
        "    # X_holdout_scaled = scaler_final.transform(X_holdout)\n",
        "    # final_model, mae_ho, rmse_ho, _ = train_evaluate_model(\n",
        "    #     pd.DataFrame(X_train_cv_scaled, columns=X_train_cv.columns, index=X_train_cv.index), y_train_cv,\n",
        "    #     pd.DataFrame(X_holdout_scaled, columns=X_holdout.columns, index=X_holdout.index), y_holdout,\n",
        "    #     XGB_PARAMS, model_name=\"Final Full Model\"\n",
        "    # )\n",
        "\n",
        "    final_model, mae_ho, rmse_ho, _ = train_evaluate_model(\n",
        "        X_train_cv, y_train_cv,\n",
        "        X_holdout, y_holdout,\n",
        "        XGB_PARAMS, model_name=\"Final Full Model\"\n",
        "    )\n",
        "    print(f\"Performance do Modelo Final no Hold-out: MAE = {mae_ho:.3f}, RMSE = {rmse_ho:.3f}\")\n",
        "\n",
        "    # --- 8. Permutation Importance (R11) ---\n",
        "    # Usar o modelo final treinado e o conjunto de hold-out (ou um conjunto de valida√ß√£o separado do CV)\n",
        "    # Se o hold-out for muito pequeno, pode-se usar o √∫ltimo fold do CV para isso,\n",
        "    # mas requer refazer o split ou guardar os dados do √∫ltimo fold.\n",
        "    # Usaremos o hold-out aqui.\n",
        "    if not X_holdout.empty and not y_holdout.empty:\n",
        "        # Se escalou para o modelo final, precisa usar X_holdout_scaled\n",
        "        # df_perm_importance = get_permutation_feature_importance(final_model, X_holdout_scaled, y_holdout)\n",
        "        df_perm_importance = get_permutation_feature_importance(final_model, X_holdout, y_holdout)\n",
        "        print(\"\\nImport√¢ncia das Features (Permutation) no Hold-out:\")\n",
        "        print(df_perm_importance)\n",
        "    else:\n",
        "        print(\"Aviso: Conjunto de Hold-out vazio, pulando Permutation Importance.\")\n",
        "\n",
        "\n",
        "    # --- 9. Refinamento e Conclus√µes (R12) ---\n",
        "    print(\"\\n--- Conclus√µes e Pr√≥ximos Passos ---\")\n",
        "    print(\"O pipeline foi executado.\")\n",
        "    print(\"Analise os resultados de MAE/RMSE, os testes estat√≠sticos e a import√¢ncia das features.\")\n",
        "    print(\"Pr√≥ximos passos podem incluir:\")\n",
        "    print(\"  - Otimiza√ß√£o de hiperpar√¢metros do XGBoost.\")\n",
        "    print(\"  - Explora√ß√£o de diferentes tamanhos de janela e features.\")\n",
        "    print(\"  - Investiga√ß√£o mais aprofundada da estima√ß√£o de 'd' para ARFIMA.\")\n",
        "    print(\"  - Remo√ß√£o de features com baixa import√¢ncia (conforme R12).\")\n",
        "    print(\"  - Implementa√ß√£o de Monte Carlo CV para resultados mais robustos (conforme R7).\")\n",
        "\n",
        "    print(\"\\n--- Fim do Pipeline ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Adicionar um try-except para capturar erros de forma mais elegante\n",
        "    try:\n",
        "        main()\n",
        "    except ValueError as ve:\n",
        "        print(f\"Erro de Valor no pipeline: {ve}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Um erro inesperado ocorreu: {e}\")\n",
        "        # Opcional: raise e para ver o traceback completo para debugging\n",
        "        # raise e"
      ]
    }
  ]
}